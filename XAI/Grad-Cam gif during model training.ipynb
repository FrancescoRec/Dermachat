{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4aac155-077a-4c32-9c71-c49de29cefa8",
   "metadata": {},
   "source": [
    "## Model Training with Grad-CAM:\n",
    "\n",
    "The purpose of this code is to verify the Xception model's behavior during training. The function \"train_model_with_gradcam\" integrates a Grad-CAM (Gradient-weighted Class Activation Mapping) layer into the training process, enabling the generation of Grad-CAM images during validation. By saving these images, we can monitor the model's focus over time, ensuring that it learns to concentrate on skin lesions rather than irrelevant artifacts.\n",
    "\n",
    "Grad-CAM captures activations and gradients from a designated layer of the Xception model to compute and visualize class activation maps. This provides insights into the model's decision-making process and helps verify that the model is focusing on relevant features.\n",
    "\n",
    "The code also creates a GIF from the saved Grad-CAM images, providing a dynamic view of the model's focus across different training stages. This visual representation confirms that the model consistently pays attention to the relevant areas, enhancing the interpretability and reliability of the model's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c7cc3ae-3752-4529-b780-d02681b3f010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.6349 Acc: 0.6707\n",
      "val Loss: 0.5501 Acc: 0.7557\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.5296 Acc: 0.7500\n",
      "val Loss: 0.4796 Acc: 0.7786\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.4336 Acc: 0.8171\n",
      "val Loss: 0.3850 Acc: 0.8321\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.3443 Acc: 0.8567\n",
      "val Loss: 0.2918 Acc: 0.8779\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.3082 Acc: 0.8628\n",
      "val Loss: 0.2659 Acc: 0.8855\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.2609 Acc: 0.8765\n",
      "val Loss: 0.2244 Acc: 0.9160\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.2561 Acc: 0.8994\n",
      "val Loss: 0.1798 Acc: 0.9389\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.1961 Acc: 0.9192\n",
      "val Loss: 0.2199 Acc: 0.9160\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.2204 Acc: 0.9131\n",
      "val Loss: 0.1776 Acc: 0.9466\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.1770 Acc: 0.9375\n",
      "val Loss: 0.1665 Acc: 0.9313\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.1850 Acc: 0.9375\n",
      "val Loss: 0.1575 Acc: 0.9389\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.1929 Acc: 0.9314\n",
      "val Loss: 0.1892 Acc: 0.9466\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.2059 Acc: 0.9268\n",
      "val Loss: 0.1751 Acc: 0.9466\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.1890 Acc: 0.9329\n",
      "val Loss: 0.1571 Acc: 0.9389\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.1474 Acc: 0.9466\n",
      "val Loss: 0.1649 Acc: 0.9542\n",
      "Best val Acc: 0.954198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17552\\571831815.py:224: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread(filename))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import timm\n",
    "import copy\n",
    "import os\n",
    "import cv2\n",
    "import imageio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDDIDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.ddi_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.ddi_frame['malignant'] = self.ddi_frame['malignant'].astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ddi_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.ddi_frame.iloc[idx]['DDI_file'])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.ddi_frame.iloc[idx]['malignant']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "# Define transformations without normalization\n",
    "train_transform = T.Compose([\n",
    "    T.Resize(299),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.RandomResizedCrop(299),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize(299),\n",
    "    T.CenterCrop(299),\n",
    "    T.ToTensor()\n",
    "])\n",
    "# Initialize datasets\n",
    "full_dataset = CustomDDIDataset(csv_file='C:\\\\Users\\\\user\\\\DDI\\\\ddi_metadata.csv',\n",
    "                                root_dir='C:\\\\Users\\\\user\\\\DDI\\\\images',\n",
    "                                transform=train_transform)\n",
    "\n",
    "train_dataset = CustomDDIDataset(csv_file='C:\\\\Users\\\\user\\\\DDI\\\\ddi_metadata.csv',\n",
    "                                 root_dir='C:\\\\Users\\\\user\\\\DDI\\\\images',\n",
    "                                 transform=train_transform)\n",
    "\n",
    "val_dataset = CustomDDIDataset(csv_file='C:\\\\Users\\\\user\\\\DDI\\\\ddi_metadata.csv',\n",
    "                               root_dir='C:\\\\Users\\\\user\\\\DDI\\\\images',\n",
    "                               transform=val_transform)\n",
    "\n",
    "# Handle class imbalance\n",
    "ddi_df = pd.read_csv('C:\\\\Users\\\\user\\\\DDI\\\\ddi_metadata.csv')\n",
    "labels = ddi_df['malignant'].values\n",
    "class_sample_count = torch.tensor([\n",
    "    (labels == 0).sum(),\n",
    "    (labels == 1).sum()\n",
    "])\n",
    "\n",
    "class_weights = 1. / class_sample_count.float()\n",
    "samples_weights = np.array([class_weights[int(label)] for label in labels])  # Ensure label is int\n",
    "samples_weights = torch.tensor(samples_weights)\n",
    "\n",
    "sampler = WeightedRandomSampler(weights=samples_weights, num_samples=len(samples_weights), replacement=True)\n",
    "\n",
    "# Split dataset\n",
    "train_idx, val_test_idx = train_test_split(range(len(full_dataset)), test_size=0.4, random_state=42)\n",
    "val_idx, test_idx = train_test_split(val_test_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "train_subset = Subset(full_dataset, train_idx)\n",
    "val_subset = Subset(full_dataset, val_idx)\n",
    "test_subset = Subset(full_dataset, test_idx)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler, shuffle=False)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Grad-CAM class with register_full_backward_hook and handling division by zero\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.model.eval()\n",
    "        self.hook_layers()\n",
    "\n",
    "    def hook_layers(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "\n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    def generate_cam(self, input_image, target_class=None, epsilon=1e-8):\n",
    "        output = self.model(input_image)\n",
    "        self.model.zero_grad()\n",
    "        if target_class is None:\n",
    "            target_class = output.argmax().item()\n",
    "        target = output[:, target_class]\n",
    "        target.backward()\n",
    "        \n",
    "        gradients = self.gradients.data.numpy()[0]\n",
    "        activations = self.activations.data.numpy()[0]\n",
    "        \n",
    "        weights = np.mean(gradients, axis=(1, 2))\n",
    "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "        \n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "        \n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (input_image.shape[2], input_image.shape[3]))\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / (np.max(cam) + epsilon)  # Avoid division by zero\n",
    "        return cam\n",
    "\n",
    "def apply_colormap_on_image(org_im, activation, colormap_name='jet'):\n",
    "    color_map = cv2.applyColorMap(np.uint8(255 * activation), cv2.COLORMAP_JET)\n",
    "    color_map = np.float32(color_map) / 255\n",
    "    cam = color_map + np.float32(org_im)\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)\n",
    "\n",
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)\n",
    "\n",
    "\n",
    "# Training function with Grad-CAM saving\n",
    "def train_model_with_gradcam(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=15, patience=3):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    target_layer = model.conv4  # Choose an appropriate layer from the model\n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Save Grad-CAM images\n",
    "            if phase == 'val':\n",
    "                for i, (inputs, labels) in enumerate(dataloader):\n",
    "                    inputs = inputs.to(device)\n",
    "                    img_tensor = inputs[0].unsqueeze(0)  # Take the first image\n",
    "                    cam = grad_cam.generate_cam(img_tensor)\n",
    "                    cam_image = show_cam_on_image(inputs[0].cpu().numpy().transpose(1, 2, 0), cam)\n",
    "                    Image.fromarray(cam_image).save(f'gradcam_epoch_{epoch}_image_{i}.png')\n",
    "                    if i == 5:  # Save only a few images per epoch for example\n",
    "                        break\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Create GIF\n",
    "def create_gif(output_path, pattern='gradcam_epoch_*.png'):\n",
    "    images = []\n",
    "    for filename in sorted(glob.glob(pattern), key=os.path.getmtime):\n",
    "        images.append(imageio.imread(filename))\n",
    "    imageio.mimsave(output_path, images, fps=2)\n",
    "\n",
    "# Initialize model, criterion, optimizer, and scheduler\n",
    "xception_model = timm.create_model('legacy_xception', pretrained=True)  # Use the correct model name\n",
    "num_ftrs = xception_model.fc.in_features  # The final fully connected layer\n",
    "xception_model.fc = nn.Linear(num_ftrs, 2)  # Replace with a new fully connected layer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xception_model = xception_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(xception_model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Train the model and generate Grad-CAM images\n",
    "Xception_best_model = train_model_with_gradcam(xception_model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=15, patience=3)\n",
    "\n",
    "# Create GIF from saved images\n",
    "create_gif('gradcam_model_training.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
